{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T04:52:59.072629Z",
     "start_time": "2025-02-25T04:52:59.013347Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# question 4\n",
    "\n",
    "df = pd.read_csv('./data/Question2_Dataset.csv', header=None,\n",
    "                 names=['X1', 'X2', 'X3', 'X4', 'Y'])\n",
    "\n",
    "m = df.shape[0]\n",
    "# Extract features and target\n",
    "X = df[['X1', 'X2', 'X3']].values\n",
    "y = df['Y'].values\n",
    "\n",
    "# Add an intercept (bias) term (a column of ones)\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# Define helper functions\n",
    "# -----------------------------\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost for linear regression.\n",
    "    NOTE: Here we use 1/m (not 1/(2*m)) so that the cost\n",
    "    matches the values in your reference table.\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/m) * np.sum((predictions - y)**2)\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to learn theta.\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    for _ in range(iterations):\n",
    "        predictions = X.dot(theta)\n",
    "        error = predictions - y\n",
    "        # Gradient descent update\n",
    "        theta = theta - (alpha/m) * (X.T.dot(error))\n",
    "        cost_history.append(compute_cost(X, y, theta))\n",
    "    return theta, cost_history\n",
    "\n",
    "def run_experiment(iterations):\n",
    "    \"\"\"\n",
    "    Run gradient descent for a given number of iterations and\n",
    "    return the final cost, theta values, r2 score, and predictions.\n",
    "    \"\"\"\n",
    "    alpha = 0.1\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)\n",
    "    final_cost = cost_history[-1]\n",
    "    predictions = X.dot(theta)\n",
    "    r2 = r2_score(y, predictions)\n",
    "    return final_cost, theta, r2, predictions\n",
    "\n",
    "# -----------------------------\n",
    "# Run experiments for different iterations\n",
    "# -----------------------------\n",
    "results = {}\n",
    "for n in [1, 10, 100, 1000]:\n",
    "    cost, theta_vals, r2, preds = run_experiment(n)\n",
    "    results[n] = (cost, theta_vals, r2, preds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# question 5\n",
    "\n",
    "# We take the columns Penalty, FreeKick, Corner as features.\n",
    "X = df[['X1', 'X2', 'X3']].values\n",
    "y = df['Y'].values\n",
    "\n",
    "# Number of training examples\n",
    "m = len(y)\n",
    "\n",
    "# Add a column of ones for the intercept term\n",
    "# so that X becomes [1, Penalty, FreeKick, Corner]\n",
    "X = np.hstack([np.ones((m, 1)), X])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Compute theta using the Normal Equation:\n",
    "#    theta = (X^T * X)^(-1) * X^T * y\n",
    "# ---------------------------------------------------------------------\n",
    "# Note: In practice, you may want to use a pseudo-inverse for numerical stability.\n",
    "theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Make predictions and compute r^2 score\n",
    "# ---------------------------------------------------------------------\n",
    "predictions = X @ theta\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "\n",
    "# question 4 results\n",
    "print(\"{:<12} {:<14} {:<40} {:<8}\".format(\n",
    "    \"# iterations\",\n",
    "    \"Cost Function\",\n",
    "    \"Optimal Values of Theta\",\n",
    "    \"r2_score\"\n",
    "))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Data rows\n",
    "for n in [1, 10, 100, 1000]:\n",
    "    cost, theta_vals, r2, _ = results[n]\n",
    "    cost_str = f\"{cost:.3f}\"\n",
    "    theta_str = \", \".join([f\"{theta:.3f}\" for theta in theta_vals])\n",
    "    r2_str = f\"{r2:.3f}\"\n",
    "    print(f\"n = {n:<10} {cost_str:<14} {theta_str:<40} {r2_str:<8}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Determine the player with the maximum predicted value after 1000 iterations\n",
    "# -----------------------------\n",
    "_, theta_1000, _, preds_1000 = results[1000]\n",
    "max_index = np.argmax(preds_1000)\n",
    "player_max = df.iloc[max_index]['Player']\n",
    "print(f\"\\nWho has a maximum predicted value after 1000 iterations? {player_max}\")\n",
    "\n",
    "\n",
    "# question 5 results\n",
    "print(\"Optimal values of theta (rounded to 3 decimals):\")\n",
    "for i, val in enumerate(theta):\n",
    "    print(f\"theta{i} = {val:.3f}\")\n",
    "\n",
    "print(f\"\\nr2-score value (rounded to 3 decimals): {r2:.3f}\")"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 64\u001B[0m\n\u001B[0;32m     62\u001B[0m results \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m1000\u001B[39m]:\n\u001B[1;32m---> 64\u001B[0m     cost, theta_vals, r2, preds \u001B[38;5;241m=\u001B[39m \u001B[43mrun_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m     results[n] \u001B[38;5;241m=\u001B[39m (cost, theta_vals, r2, preds)\n\u001B[0;32m     70\u001B[0m \u001B[38;5;66;03m# question 5\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \n\u001B[0;32m     72\u001B[0m \u001B[38;5;66;03m# We take the columns Penalty, FreeKick, Corner as features.\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[4], line 53\u001B[0m, in \u001B[0;36mrun_experiment\u001B[1;34m(iterations)\u001B[0m\n\u001B[0;32m     51\u001B[0m alpha \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.1\u001B[39m\n\u001B[0;32m     52\u001B[0m theta \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m---> 53\u001B[0m theta, cost_history \u001B[38;5;241m=\u001B[39m \u001B[43mgradient_descent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtheta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterations\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m final_cost \u001B[38;5;241m=\u001B[39m cost_history[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     55\u001B[0m predictions \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mdot(theta)\n",
      "Cell \u001B[1;32mIn[4], line 39\u001B[0m, in \u001B[0;36mgradient_descent\u001B[1;34m(X, y, theta, alpha, iterations)\u001B[0m\n\u001B[0;32m     37\u001B[0m cost_history \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(iterations):\n\u001B[1;32m---> 39\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtheta\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m     error \u001B[38;5;241m=\u001B[39m predictions \u001B[38;5;241m-\u001B[39m y\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;66;03m# Gradient descent update\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
